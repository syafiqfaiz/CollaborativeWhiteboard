# Lesson 3: Backend Infrastructure - Custom Servers vs PaaS, Scaling, and Sessions

## Learning Objectives
By the end of this lesson, you'll understand:
- The trade-offs between custom WebSocket servers and Platform-as-a-Service (PaaS)
- Why we chose Supabase Realtime over building our own server
- How sticky sessions work in WebSocket architectures
- Scaling strategies for real-time applications

---

## 1. Architecture Decision: Custom Server vs PaaS

### Option 1: Custom WebSocket Server (Node.js/Go)

**Example Stack:**
```
Frontend ‚Üí Load Balancer ‚Üí WebSocket Server (Node.js + Socket.io)
                                    ‚Üì
                              Redis Pub/Sub
                                    ‚Üì
                           PostgreSQL (optional)
```

**Pros:**
- ‚úÖ Full control over logic
- ‚úÖ Custom optimizations
- ‚úÖ No vendor lock-in

**Cons:**
- ‚ùå Infrastructure management (servers, load balancers, Redis)
- ‚ùå Scaling complexity (sticky sessions, horizontal scaling)
- ‚ùå Monitoring & debugging
- ‚ùå Cost (server hosting, DevOps time)

### Option 2: PaaS (Supabase Realtime) ‚úÖ Our Choice

**Example Stack:**
```
Frontend ‚Üí Supabase Realtime (Managed WebSocket Service)
```

**Pros:**
- ‚úÖ Zero infrastructure management
- ‚úÖ Built-in scaling
- ‚úÖ Free tier (generous limits)
- ‚úÖ Presence tracking included
- ‚úÖ Fast time-to-market

**Cons:**
- ‚ö†Ô∏è Vendor lock-in
- ‚ö†Ô∏è Less control over internals
- ‚ö†Ô∏è Pricing at scale (can get expensive)

---

## 2. Our Implementation: Supabase Client

### Initialization

**File:** `src/lib/supabase.ts`

```typescript
import { createClient } from '@supabase/supabase-js';

const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;
const supabaseKey = import.meta.env.VITE_SUPABASE_KEY;

export const supabase = createClient(
  supabaseUrl || 'https://placeholder.supabase.co',
  supabaseKey || 'placeholder',
);
```

**What's happening:**
1. Import Supabase JavaScript client
2. Read credentials from environment variables
3. Create singleton client instance

### Environment Configuration

**File:** `.env.example`

```bash
VITE_SUPABASE_URL=your_supabase_url_here
VITE_SUPABASE_KEY=your_supabase_anon_key_here
```

**Security Note:**
- `VITE_SUPABASE_KEY` is the **anon public key** (safe to expose)
- Real security comes from Row Level Security (RLS) policies
- For our use case, we don't use RLS (no database)

---

## 3. What Supabase Realtime Does Behind the Scenes

### Under the Hood

Supabase Realtime is built on **Phoenix Framework** (Elixir):

```
Your Browser ‚Üí WebSocket ‚Üí Phoenix Channels ‚Üí Broadcast to all subscribers
                                    ‚Üì
                            (No database writes)
```

**Key Components:**
1. **Phoenix Channels**: Elixir's pub/sub system
2. **Presence**: Distributed user tracking (CRDT-based)
3. **Broadcast**: Fire-and-forget messaging

### Why Elixir/Phoenix?

- **Concurrency**: Handles millions of connections on one server
- **Fault Tolerance**: Processes crash independently
- **Low Latency**: ~10-50ms message delivery
- **Battle-tested**: Used by Discord, Pinterest, etc.

---

## 4. If We Built a Custom Server (Hypothetical)

### Node.js + Socket.io Example

```javascript
// server.js
const express = require('express');
const http = require('http');
const socketIO = require('socket.io');
const Redis = require('ioredis');

const app = express();
const server = http.createServer(app);
const io = socketIO(server);

// Redis for pub/sub across multiple servers
const pub = new Redis();
const sub = new Redis();

io.on('connection', (socket) => {
  const roomId = socket.handshake.query.room || 'room-1';
  
  // Join room
  socket.join(roomId);
  
  // Listen for draw events
  socket.on('draw-line', (stroke) => {
    // Broadcast to room (including other servers via Redis)
    pub.publish(`room:${roomId}:draw`, JSON.stringify(stroke));
  });
  
  // Listen for cursor moves
  socket.on('cursor-move', (cursor) => {
    socket.to(roomId).emit('cursor-move', cursor);
  });
});

// Subscribe to Redis for cross-server messages
sub.subscribe('room:*:draw');
sub.on('message', (channel, message) => {
  const roomId = channel.split(':')[1];
  io.to(roomId).emit('draw-line', JSON.parse(message));
});

server.listen(3000);
```

**Complexity Added:**
- Redis for cross-server communication
- Room management logic
- Connection handling
- Error handling & reconnection logic
- Monitoring & logging

**Estimated Development Time:** 2-4 weeks  
**Supabase Setup Time:** 10 minutes

---

## 5. Sticky Sessions Problem

### What are Sticky Sessions?

When you have **multiple WebSocket servers**, you need to ensure a user's connection stays on the **same server**.

```
User A ‚Üí Load Balancer ‚Üí Server 1 (WebSocket connection)
User B ‚Üí Load Balancer ‚Üí Server 2 (WebSocket connection)
```

**Problem:**
- User A sends message
- Server 1 receives it
- Server 1 needs to send to User B
- But User B is on Server 2!

### Solutions

#### 1. Sticky Sessions (Session Affinity)

```
Load Balancer remembers: User A ‚Üí Server 1
                         User B ‚Üí Server 2
```

**Pros:**
- ‚úÖ Simple to implement

**Cons:**
- ‚ùå Uneven load distribution
- ‚ùå Server restart disconnects users

#### 2. Redis Pub/Sub (Our Hypothetical Example)

```
Server 1 ‚Üí Redis Pub/Sub ‚Üí Server 2
```

**Pros:**
- ‚úÖ Servers can communicate
- ‚úÖ No sticky sessions needed

**Cons:**
- ‚ùå Additional infrastructure (Redis)
- ‚ùå Latency overhead (~5-10ms)

#### 3. Managed Service (Supabase) ‚úÖ

Supabase handles this internally with **Elixir's distributed nodes**.

---

## 6. Scaling Strategies

### Vertical Scaling (Scale Up)

```
1 Server: 4 CPU, 8GB RAM ‚Üí 16 CPU, 64GB RAM
```

**Limits:**
- Single server: ~50,000 concurrent connections
- Expensive at high specs

### Horizontal Scaling (Scale Out)

```
1 Server ‚Üí 5 Servers ‚Üí 50 Servers
```

**Challenges:**
- Need load balancer
- Need message broker (Redis/RabbitMQ)
- Sticky sessions or pub/sub

### Supabase Auto-Scaling

Supabase handles scaling automatically:
- **Free Tier**: Up to 200 concurrent connections
- **Pro Tier**: Up to 500 concurrent connections
- **Enterprise**: Custom limits

**For our use case:**
- 5 users per room √ó 40 rooms = 200 connections (Free tier ‚úÖ)

---

## 7. Cost Comparison

### Custom Server (AWS Example)

| Component | Cost/Month |
|-----------|------------|
| EC2 t3.medium (2 servers) | $60 |
| Application Load Balancer | $20 |
| ElastiCache Redis | $15 |
| CloudWatch Monitoring | $10 |
| **Total** | **$105/month** |

**Plus:**
- DevOps time (~10 hours/month)
- Maintenance & updates

### Supabase

| Tier | Cost/Month | Connections |
|------|------------|-------------|
| Free | $0 | 200 |
| Pro | $25 | 500 |
| **Total** | **$0-25/month** | Enough for MVP |

**Plus:**
- Zero DevOps time
- Automatic updates

---

## 8. When to Build Custom vs Use PaaS

### Use PaaS (Supabase, Pusher, Ably) When:

- ‚úÖ MVP or small-scale app
- ‚úÖ Standard use cases (chat, notifications, collaboration)
- ‚úÖ Limited DevOps resources
- ‚úÖ Fast time-to-market critical

### Build Custom Server When:

- ‚úÖ Very high scale (millions of connections)
- ‚úÖ Custom business logic in real-time layer
- ‚úÖ Cost optimization at scale (PaaS gets expensive)
- ‚úÖ Regulatory requirements (data sovereignty)

### Our Decision Matrix

| Factor | Weight | Custom | Supabase |
|--------|--------|--------|----------|
| Time to market | High | ‚ùå 4 weeks | ‚úÖ 1 day |
| Cost (MVP) | High | ‚ùå $105/mo | ‚úÖ $0/mo |
| Scalability | Low | ‚úÖ Unlimited | ‚ö†Ô∏è 500 users |
| Control | Low | ‚úÖ Full | ‚ùå Limited |
| **Winner** | | | **Supabase** ‚úÖ |

---

## 9. Monitoring & Debugging

### Supabase Dashboard

Supabase provides built-in monitoring:
- Real-time connection count
- Message throughput
- Error logs
- Presence state

### Custom Server Monitoring

Would need to implement:
- Prometheus metrics
- Grafana dashboards
- Error tracking (Sentry)
- Log aggregation (ELK stack)

**Estimated setup time:** 1-2 weeks

---

## 10. Migration Path (If We Outgrow Supabase)

### Step 1: Add Abstraction Layer

```typescript
// realtime.service.ts
interface RealtimeService {
  subscribe(roomId: string): Channel;
  broadcast(event: string, payload: any): void;
  track(presence: any): void;
}

class SupabaseRealtimeService implements RealtimeService {
  // Current implementation
}

class CustomRealtimeService implements RealtimeService {
  // Future custom implementation
}
```

### Step 2: Gradual Migration

1. Deploy custom server alongside Supabase
2. Route 10% of traffic to custom server
3. Monitor performance & errors
4. Gradually increase to 100%
5. Deprecate Supabase

**Estimated migration time:** 2-3 months

---

## 11. Code Exercise

**Challenge:** Add connection status indicator

```typescript
// useWhiteboard.ts
const [connectionStatus, setConnectionStatus] = useState<'connecting' | 'connected' | 'disconnected'>('connecting');

channel.subscribe((status) => {
  if (status === 'SUBSCRIBED') {
    setConnectionStatus('connected');
  } else if (status === 'CLOSED') {
    setConnectionStatus('disconnected');
  }
});

// In UI component
{connectionStatus === 'connected' && <span>üü¢ Online</span>}
{connectionStatus === 'disconnected' && <span>üî¥ Offline</span>}
```

---

## 12. Key Takeaways

| Concept | What We Learned |
|---------|----------------|
| **PaaS vs Custom** | PaaS wins for MVPs, custom for scale/control |
| **Supabase Realtime** | Managed WebSocket service built on Phoenix/Elixir |
| **Sticky Sessions** | Challenge in multi-server WebSocket deployments |
| **Scaling** | Vertical (bigger servers) vs Horizontal (more servers) |
| **Cost** | Supabase $0-25/mo vs Custom $105+/mo |
| **Migration** | Abstract early to enable future migration |

---

## Next Steps

In **Lesson 4**, we'll explore data structures: vector vs raster storage, coordinate normalization, and payload compression techniques.
